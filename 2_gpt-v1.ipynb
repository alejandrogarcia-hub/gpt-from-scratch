{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT v1\n",
    "-------------------\n",
    "Based on the following attention architecture ...</br>\n",
    "<img src=\"draws/attention-gpt.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 28 17:00:31 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.65                 Driver Version: 551.86         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  |   00000000:01:00.0  On |                    0 |\n",
      "| 31%   37C    P0             53W /  450W |    1819MiB /  23028MiB |      1%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from markdown import markdown\n",
    "import mmap\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypterparameters\n",
    "# number fo characters per batch\n",
    "block_size = 32\n",
    "batch_size = 128\n",
    "epochs = 2100\n",
    "learning_rate = 3e-4\n",
    "eval_inters = 100\n",
    "eval_interval = 300\n",
    "# number fo features\n",
    "embedding_dim = 256\n",
    "# for the multi head attention layer\n",
    "n_heads = 4\n",
    "# number of decoders\n",
    "n_layers = 4\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from file\n",
    "---\n",
    "I used the file ```data/train_data_1.txt```, which is all the openwebtext in one file. It is too big to be loaded in memory, therefore, we chunk it. </br>\n",
    "**And** what I did was running the ```read_data.py```, it will generate all the required files:\n",
    "- ```data/train_data_1.txt```\n",
    "- ```data/vocab.txt```\n",
    "Now,\n",
    "1.  load the vocabulary so we can create the encode and decode functions.\n",
    "2. load the data in chunks \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28477"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data/vocab.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chars = sorted(list(set(f.read())))\n",
    "    vocab_size = len(chars)\n",
    "\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### char encoder and decoder\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then, create the mappings\n",
    "string_to_int = {string: i for i, string in enumerate(chars)}\n",
    "int_to_string = {i: string for i, string in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, convert the book to integers\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda s: \"\".join([int_to_string[c] for c in s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### short test the encoder and decoder\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "encoded_string = encode(\"hello\")\n",
    "decoded_string = decode(encoded_string)\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read data in chunks\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_chunk(filname: str, block_size: int, batch_size: int) -> str:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        filname (str): _description_\n",
    "        block_size (int): _description_\n",
    "        batch_size (int): _description_\n",
    "\n",
    "    Returns:\n",
    "        str: _description_\n",
    "    \"\"\"\n",
    "    with open(filname, \"rb\") as f:\n",
    "        # mmap == memory mapping, does not open the whole file at once\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as m:\n",
    "            file_size = m.size()\n",
    "            start_index = random.randint(0, (file_size) - block_size * batch_size)\n",
    "            m.seek(start_index)\n",
    "            block = m.read(block_size * batch_size - 1)\n",
    "            # we decoded it becuase we load it in binary format\n",
    "            decoded_block = block.decode(\"utf-8\", errors=\"ignore\").replace(\"\\r\", \"\")\n",
    "\n",
    "            data = torch.tensor(encode(decoded_block), dtype=torch.long)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(filname: str, block_size: int, batch_size: int):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        filname (str): _description_\n",
    "        block_size (int): _description_\n",
    "        batch_size (int): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    chunk = get_random_chunk(filname, block_size, batch_size)\n",
    "    starts = torch.randint(len(chunk) - block_size, (batch_size,))\n",
    "    x = torch.stack([chunk[i : i + block_size] for i in starts]).to(device)\n",
    "    y = torch.stack([chunk[i + 1 : i + block_size + 1] for i in starts]).to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_batch(\"data/train_data_1.txt\", block_size, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_loss(model, block_size):\n",
    "    out = {}\n",
    "    _ = model.eval()\n",
    "\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_inters)\n",
    "        for i in range(eval_inters):\n",
    "            # data = train if split == \"train\" else val\n",
    "            input_batch, target_batch = get_batch(\n",
    "                \"data/train_data_1.txt\", block_size, batch_size\n",
    "            )\n",
    "            _, loss = model.forward(input_batch, target_batch)\n",
    "            losses[i] = loss.item()\n",
    "\n",
    "        out[split] = losses.mean()\n",
    "\n",
    "    _ = model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GTP Model\n",
    "--------------------\n",
    "It follow the simplified GPT attention architecture. You can read about it [page 9](https://arxiv.org/pdf/2305.10435v1).</br>\n",
    "It looks compicated, but it is not. Is quite simple just a lot of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullMultiheadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, n_heads, dropout):\n",
    "        super(FullMultiheadAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = embedding_dim // n_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.k_linear = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.v_linear = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # get number of training examples\n",
    "        n_batches = query.shape[0]\n",
    "\n",
    "        # perform linear transformation and split into N heads\n",
    "        query = self.q_linear(query).view(n_batches, -1, self.n_heads, self.head_dim)\n",
    "        key = self.k_linear(key).view(n_batches, -1, self.n_heads, self.head_dim)\n",
    "        value = self.v_linear(value).view(n_batches, -1, self.n_heads, self.head_dim)\n",
    "\n",
    "        # transpose to get dimensions batch_size * n_heads * seq_len * head_dim\n",
    "        query = query.transpose(1, 2)\n",
    "        key = key.transpose(1, 2)\n",
    "        value = value.transpose(1, 2)\n",
    "\n",
    "        # calculate attention using function we will define next\n",
    "        scores = self.attention(query, key, value)\n",
    "\n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = (\n",
    "            scores.transpose(1, 2).contiguous().view(n_batches, -1, self.embedding_dim)\n",
    "        )\n",
    "        output = self.out(concat)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(\n",
    "            torch.tensor(self.head_dim, dtype=torch.float32)\n",
    "        )\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        scores = self.dropout(scores)\n",
    "        output = torch.matmul(scores, value)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.k = nn.Linear(embedding_dim, n_heads, bias=False)\n",
    "        self.q = nn.Linear(embedding_dim, n_heads, bias=False)\n",
    "        self.v = nn.Linear(embedding_dim, n_heads, bias=False)\n",
    "        # registers the no look ahead masking in the model state,\n",
    "        # preventss overhead computation of having to read over and over again\n",
    "        # we can train without it, but it will take longer\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input size is batch (b), block_size (t), vocab_size (c)\n",
    "        # output size is batch (b), block_size (t), n_heads (h)\n",
    "        b, t, c = x.shape\n",
    "        k = self.k(x)\n",
    "        q = self.q(x)\n",
    "        v = self.v(x)\n",
    "\n",
    "        # compute attention scores (\"affinities\"), w stand for weights\n",
    "        # q * k^T / sqrt(h), the multi of q and k and then applied the scale factor (reference architecture)\n",
    "        # scaling helps on hearing all part of the conversation, not just the loudest\n",
    "\n",
    "        # (b,t,h) @ (b,h,t) -> (b,t,t)\n",
    "        w = q @ k.transpose(-2, -1) * k.shape[-1] ** (-0.5)\n",
    "        # masking so netwrok does not look ahead an cheat (reference architecture)\n",
    "        # (b, t, t)\n",
    "        w = w.masked_fill(self.tril[:t, :t] == 0, float(\"-inf\"))\n",
    "        # (b, t, t)\n",
    "        w = F.softmax(w, dim=-1)\n",
    "        w = self.dropout(w)\n",
    "        # perform weighted aggergation of the values\n",
    "        # (b, t, h)\n",
    "        v = self.v(x)\n",
    "        # (b, t, t) @ (b, t, h) -> (b, t, h\n",
    "        return w @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_heads, head_size, embedding_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # ModuleList, independent modukes that can be run in parallel.\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(embedding_dim, head_size, dropout) for _ in range(n_heads)]\n",
    "        )\n",
    "        self.proj = nn.Linear(n_heads * head_size, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # concat along the last dimmension, which is the feature dimmension\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return self.dropout(self.proj(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embedding_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, embedding_dim: int, n_heads: int, head_size: int, dropout: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # Multihead attention head dim, is calcualted as:\n",
    "        # head_dim = embedding_dim // n_head\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html\n",
    "        # self.multihead_attention = nn.MultiheadAttention(embedding_dim, n_head)\n",
    "        self.multihead_attention = MultiheadAttention(\n",
    "            n_heads, head_size, embedding_dim, dropout=dropout\n",
    "        )\n",
    "        # self.ff = nn.Sequential(\n",
    "        #     nn.Linear(embedding_dim, n_layers * embedding_dim),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(n_layers * embedding_dim, embedding_dim),\n",
    "        #     nn.Dropout(dropout),\n",
    "        # )\n",
    "        self.ff = FeedForward(embedding_dim, 4 * embedding_dim, dropout)\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.multihead_attention(x)\n",
    "        x = self.norm1(x + y)\n",
    "        y = self.ff(x)\n",
    "        x = self.norm2(x + y)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTLanguageModel(\n",
       "  (char_embeddings): Embedding(28477, 256)\n",
       "  (positional_encodings): Embedding(32, 256)\n",
       "  (decoders): Sequential(\n",
       "    (0): DecoderBlock(\n",
       "      (multihead_attention): MultiheadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x AttentionHead(\n",
       "            (k): Linear(in_features=256, out_features=64, bias=False)\n",
       "            (q): Linear(in_features=256, out_features=64, bias=False)\n",
       "            (v): Linear(in_features=256, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): DecoderBlock(\n",
       "      (multihead_attention): MultiheadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x AttentionHead(\n",
       "            (k): Linear(in_features=256, out_features=64, bias=False)\n",
       "            (q): Linear(in_features=256, out_features=64, bias=False)\n",
       "            (v): Linear(in_features=256, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): DecoderBlock(\n",
       "      (multihead_attention): MultiheadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x AttentionHead(\n",
       "            (k): Linear(in_features=256, out_features=64, bias=False)\n",
       "            (q): Linear(in_features=256, out_features=64, bias=False)\n",
       "            (v): Linear(in_features=256, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): DecoderBlock(\n",
       "      (multihead_attention): MultiheadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x AttentionHead(\n",
       "            (k): Linear(in_features=256, out_features=64, bias=False)\n",
       "            (q): Linear(in_features=256, out_features=64, bias=False)\n",
       "            (v): Linear(in_features=256, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (linear_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=256, out_features=28477, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        block_size: int,\n",
    "        n_heads=4,\n",
    "        n_layers=4,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.head_size = embedding_dim // n_heads\n",
    "        self.char_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # from the architecture (refer to architecture image):\n",
    "        # first positional encoding as,\n",
    "        # PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "        # PE(pos, 2i + 1) = cos(pos / 10000^(2i/d_model))\n",
    "        # where pos is the position and i is the dimension\n",
    "        # d_model is the embedding dimension\n",
    "        # exmaple: \"hello\", \"h\" will be encoded with \"sin\" and \"e\" with \"cos\"\n",
    "        # so, we need to create a matrix of shape (block_size, embedding_dim)\n",
    "        # the embeddings dimension have all the infomration required with respect to the char in that position.\n",
    "        # for GPTs we opnly use embeddings, we don't use positional encodings\n",
    "        self.positional_encodings = nn.Embedding(block_size, embedding_dim)\n",
    "\n",
    "        # Second,  define the decoder layers\n",
    "        # define decoders layer, we have 4 deacoders therefore n_layers=4\n",
    "        self.decoders = nn.Sequential(\n",
    "            *[\n",
    "                DecoderBlock(embedding_dim, n_heads, self.head_size, dropout)\n",
    "                for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Third,  linear layer with softmax activation\n",
    "        self.linear_f = nn.LayerNorm(embedding_dim)\n",
    "        # Fourth, outout as probabilities of each word in the vocab\n",
    "        self.lm_head = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "        self.apply(self._ini_weights)\n",
    "\n",
    "    # std=0.02 is used in the original implementation\n",
    "    # also is a very common value for initializing weights\n",
    "    # and it represents the std for very closed values, meaning there are no outliers\n",
    "    def _ini_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    # B, T, C the dimmensions are:\n",
    "    # (B) batch size (len(data) // block_size), (T) block_size, (C) vocab_size\n",
    "    def forward(self, index, targets=None):\n",
    "        b, t = index.shape\n",
    "        logits = self.char_embeddings(index)\n",
    "        pos_emb = self.positional_encodings(torch.arange(t, device=device))  # (T, C)\n",
    "        x = logits + pos_emb  # (B, T, C)\n",
    "        x = self.decoders(x)  # (B, T, C)\n",
    "        x = self.linear_f(x)  # (B, T, C)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits, None\n",
    "\n",
    "        b, t, c = logits.shape\n",
    "        logits = logits.view(b * t, c)\n",
    "        targets = targets.view(b * t)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens: int):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # print(index.shape, index[:, -block_size:].shape)\n",
    "            # crop it, so the max number of tokens can exceed block_size\n",
    "            index_cond = index[:, -block_size:]  # becomes (batch_size, block_size)\n",
    "            logits, _ = self.forward(index_cond)\n",
    "            logits = logits[:, -1, :]  # becomes (batch_size, n_classes)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(\n",
    "                probs, num_samples=1\n",
    "            )  # becomes (batch_size, 1)\n",
    "            index = torch.cat(\n",
    "                (index, next_token), dim=1\n",
    "            )  # becomes (batch_size, time_dim + 1)\n",
    "        return index\n",
    "\n",
    "\n",
    "model = GPTLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    block_size=block_size,\n",
    "    n_heads=n_heads,\n",
    "    n_layers=n_layers,\n",
    "    dropout=dropout,\n",
    ")\n",
    "\n",
    "# load model\n",
    "with open(\"models/gpt_v1.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, train loss: 1.9156895875930786, val loss: 1.9258555173873901\n",
      "iteration 300, train loss: 1.8608574867248535, val loss: 1.827478051185608\n",
      "iteration 600, train loss: 1.8513919115066528, val loss: 1.7979546785354614\n",
      "iteration 900, train loss: 1.8045518398284912, val loss: 1.8094267845153809\n",
      "iteration 1200, train loss: 1.779594898223877, val loss: 1.8018397092819214\n",
      "iteration 1500, train loss: 1.8209081888198853, val loss: 1.7878472805023193\n",
      "iteration 1800, train loss: 1.7546147108078003, val loss: 1.768484354019165\n",
      "final training loss, train loss: 1.7540913820266724, val loss: 1.7346044778823853\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    input_batch, target_batch = get_batch(\"data/train_data_1.txt\", block_size, batch_size)\n",
    "    # forward pass\n",
    "    logits, loss = model.forward(input_batch, target_batch)\n",
    "    # backward pass\n",
    "    # previous gradients do not affect the currrent ones, therefore, we set them to None\n",
    "    # setting the gradient to None is a memory optimization, it saves memory\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % eval_interval == 0:\n",
    "        out = evaluate_loss(model, block_size)\n",
    "        print(f\"iteration {i}, train loss: {out[\"train\"]}, val loss: {out[\"val\"]}\")\n",
    "\n",
    "out = evaluate_loss(model, block_size)\n",
    "print(f\"final training loss, train loss: {out[\"train\"]}, val loss: {out[\"val\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model\n",
    "-----\n",
    "We save it as a pickel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/gpt_v1.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat model\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello, how are evate propment around a cold chan-music surings uponey who while is blike officing mistic, the Concley’s some annought the rotown you’ve disSiumacans '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = torch.tensor(\n",
    "    encode(\"hello, how are \"), dtype=torch.long, device=device\n",
    ").unsqueeze(0)\n",
    "out = decode(model.generate(context, max_new_tokens=150)[0].tolist())\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello, how are find bunking follooms are one bl'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = decode(model.generate(context, max_new_tokens=32)[0].tolist())\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "------------\n",
    "This is just how I downloaded the ```openwebtext``` dataset. You don't need to download it, you can work with it from datasets directly. You have the example in ```1_bigram.ipynb```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the openwebtext dataset\n",
    "dataset = load_dataset(\"Skylion007/openwebtext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths to save the datasets\n",
    "train_output_file = \"data/openwebtext_train.txt\"\n",
    "\n",
    "# Save the train dataset to a text file\n",
    "with open(train_output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for sample in dataset[\"train\"]:\n",
    "        _ = f.write(sample[\"text\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the train split\n",
    "# train_dataset = dataset['train']\n",
    "val_dataset = dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Print the first sample from the train dataset\n",
    "print(train_dataset[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
