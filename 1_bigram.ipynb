{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram model\n",
    "\n",
    "---\n",
    "\n",
    "We want to build from scratch a brigram model, simple.</br>\n",
    "I used a machine with GPUs but you should be fine with running it in CPU only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from markdown import markdown\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env variables & Hyperparameters\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hypterparameters\n",
    "block_size = 64\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data\n",
    "---\n",
    "Please download the data first, from: \n",
    "- [wizard of oz](https://www.gutenberg.org/ebooks/30852), this is the one I personally used.\n",
    "- [openwebtext](https://openwebtext2.readthedocs.io)\n",
    "\n",
    "Is up to you, choose the one you like the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>\\ufeffDOROTHY AND THE WIZARD IN OZ</p>\\n<p>BY</p>\\n<p>L. FRANK BAUM</p>\\n<p>AUTHOR OF THE WIZARD OF OZ, THE LAND OF OZ, OZMA OF OZ, ETC.</p>\\n<p>ILLUSTRATED BY JOHN R. NEILL</p>\\n<p>BOOKS OF WONDER WILLIAM MORROW &amp; CO., INC. NEW YORK</p>\\n<p>[Illustration]</p>\\n<p>COPYRIGHT 1908 BY L. FRANK BAUM</p>\\n<p>ALL RIGHTS RESERVED</p>\\n<pre><code>     *       *       *       *       *\\n</code></pre>\\n<p>[Illustration]</p>\\n<p>DEDICATED TO HARRIET A. B. NEAL.</p>\\n<pre><code>     *       *       *       *       *\\n</code></pre>\\n<p>To My Readers</p>\\n<p>It\\'s no use; no use at all. The children won\\'t let me stop telling tales\\nof the Land of Oz. I know lots of other stories, and I hope to tell\\nthem, some time or another; but just now my loving tyrants won\\'t allow\\nme. They cry: \"Oz--Oz! more about Oz, Mr. Baum!\" and what can I do but\\nobey their commands?</p>\\n<p>This is Our Book--mine and the children\\'s. For they have flooded me with\\nthousands of suggestions in regard to it, and I have honestly tried to\\nadopt as many of these suggestions as could be fitted into one story.</p>\\n<p>After the wonderful success of \"Oz</p>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data/wizard_of_oz.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    book = f.read()\n",
    "\n",
    "markdown(book[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char encoder and decoder\n",
    "\n",
    "---\n",
    "We need to map the char to a numerical value and vicerversa, and, let's use the values to create the embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first, get all unique chars, and sort them\n",
    "chars = sorted(set(book))\n",
    "vocab_size = len(chars)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then, create the mappings\n",
    "string_to_int = {string: i for i, string in enumerate(chars)}\n",
    "int_to_string = {i: string for i, string in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, convert the book to integers\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda s: \"\".join([int_to_string[c] for c in s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### short test the encoder and decoder\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "encoded_string = encode(\"hello\")\n",
    "decoded_string = decode(encoded_string)\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create torch tensor\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(book), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([80, 28, 39, 42, 39, 44, 32, 49,  1, 25, 38, 28,  1, 44, 32, 29,  1, 47,\n",
       "        33, 50, 25, 42, 28,  1, 33, 38,  1, 39, 50,  0,  0,  1,  1, 26, 49,  0,\n",
       "         0,  1,  1, 36, 11,  1, 30, 42, 25, 38, 35,  1, 26, 25, 45, 37,  0,  0,\n",
       "         1,  1, 25, 45, 44, 32, 39, 42,  1, 39, 30,  1, 44, 32, 29,  1, 47, 33,\n",
       "        50, 25, 42, 28,  1, 39, 30,  1, 39, 50,  9,  1, 44, 32, 29,  1, 36, 25,\n",
       "        38, 28,  1, 39, 30,  1, 39, 50,  9,  1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and split data in train and validation datasets. </br>\n",
    "Please be careful here, of course we don't do it as is for a real solution. This is just a toy and therefore the naive split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(185846, 46462)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(0.8 * len(data))\n",
    "test_size = len(data) - train_size\n",
    "train_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232308"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "232244"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3629"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)\n",
    "len(data) - block_size\n",
    "# batches\n",
    "len(data) // block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[:train_size]\n",
    "val = data[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a fucntion to generate random batches.</br>\n",
    "Produces two vectors:\n",
    "- input, from a random starting with a length of ```block_size```\n",
    "- target, starts from input start index + 1 for a length of ```block_size```\n",
    "\n",
    "Target vector focus on the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('input: ',\n",
       " tensor([[69, 72,  1,  ..., 58,  1, 73],\n",
       "         [71, 71, 68,  ..., 55, 74, 60],\n",
       "         [56, 74, 71,  ..., 71,  1, 76],\n",
       "         ...,\n",
       "         [ 1,  1,  1,  ...,  1, 40, 33],\n",
       "         [57,  1, 62,  ..., 67, 57,  1],\n",
       "         [54, 55, 68,  ..., 62, 60, 61]], device='cuda:0'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2903, 64])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('target: ',\n",
       " tensor([[72,  1, 33,  ...,  1, 73, 68],\n",
       "         [71, 68, 76,  ..., 74, 60, 60],\n",
       "         [74, 71, 58,  ...,  1, 76, 62],\n",
       "         ...,\n",
       "         [ 1,  1,  1,  ..., 40, 33, 44],\n",
       "         [ 1, 62, 67,  ..., 57,  1, 76],\n",
       "         [55, 68, 74,  ..., 60, 61, 73]], device='cuda:0'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2903, 64])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_batches(data, block_size):\n",
    "    # generate random starting points\n",
    "    # 0 and len(data) - block_size are indexes\n",
    "    starts = torch.randint(0, len(data) - block_size, (len(data) // block_size,))\n",
    "    # from each starting point, we generate the input and target.\n",
    "    # Target is whay is supposed to be in the next sequence\n",
    "    x = torch.stack([data[start : start + block_size] for start in starts]).to(device)\n",
    "    y = torch.stack([data[start + 1 : start + block_size + 1] for start in starts]).to(\n",
    "        device\n",
    "    )\n",
    "    return x, y\n",
    "\n",
    "\n",
    "x, y = get_batches(train, block_size)\n",
    "\"input: \", x\n",
    "# first dim is the number of rows,\n",
    "# second is the number of chars we take from the starting point\n",
    "# torch.Size([23230, 8])\n",
    "x.shape\n",
    "\"target: \", y\n",
    "# torch.Size([23230, 8])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiGram Lang Model\n",
    "----------------------\n",
    "A very simple model, nothing fancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        \"\"\"We need to init our embeddings as a square matrix,\n",
    "        with each row repesenting a char and each column value the next char probability.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (_type_): _description_\n",
    "            embedding_dim (_type_): should be the same as vocab_size.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # next line is not a good idea, but it remind us the opportunity to initialize weights with an strategy\n",
    "        self.embeddings.weight.data.uniform_(-1, 1)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            index (_type_): _description_\n",
    "            targets (_type_, optional): _description_. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        logits = self.embeddings(index)\n",
    "        # next line is mainly for testing purposes\n",
    "        if targets is None:\n",
    "            return logits, None\n",
    "\n",
    "        # dimensions are:\n",
    "        # batch size (len(data) // block_size), block_size, vocab_size\n",
    "        batch_dim, time_dim, n_classes = logits.shape\n",
    "        # print(f\"embeddings shape: {logits.shape}\")\n",
    "        # batch and time are not that important, therefore, we blend them together\n",
    "        logits = logits.view(batch_dim * time_dim, n_classes)\n",
    "        # why we reshape targets? the answer is, what cross_entropy expects as shapes\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html#torch.nn.functional.cross_entropy\n",
    "        targets = targets.view(batch_dim * time_dim)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens: int):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            index (_type_): _description_\n",
    "            max_new_tokens (int): _description_\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self.forward(index)\n",
    "            # torch.Size([1, 1, 81]), torch.Size([1, 2, 81]) ... torch.Size([1, max_new_tokens, 81])\n",
    "            logits = logits[:, -1, :]  # becomes (batch_size, n_classes)\n",
    "            # print(logits.shape), torch.Size([1, 81]) as we always grab the last embeddings\n",
    "\n",
    "            # apply softmax to get probabilities, we focus on the last dimension\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution, get the index of the next char\n",
    "            next_token = torch.multinomial(\n",
    "                probs, num_samples=1\n",
    "            )  # becomes (batch_size, 1)\n",
    "            # append sample index to the running sequence, we keep on concatenating\n",
    "            # more tokens to the sequence\n",
    "            index = torch.cat(\n",
    "                (index, next_token), dim=1\n",
    "            )  # becomes (batch_size, time_dim + 1)\n",
    "        return index\n",
    "\n",
    "\n",
    "model = BigramLanguageModel(vocab_size=vocab_size, embedding_dim=vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\nZ],mF1Qz4nS\\ufeffZ\\'(L\\'3&Uetvt?IFBdR8)oX3(ZH!R(Ks!\\'otwuo3D,[j9k,*d8)\\nh2Qh\"r6Iatv]*d][nl\\nT\\ufeffpdcB;sO0h[?MLmfMIqXNYX(IX6agBU;*L\\ufeffSNzJ\"O3.u3Tl.3[.T?R:kg!\\'\\ufeff\\'[XAQRVE(pp\"9OH*LU[1,vR0V_7g69Tjzxy cp\\nDP)\\nW]An!2uc]XwNry jCI;XAaeYkY-z-&?NoHW0XOb2q7b;\"9xhr\\ufeff\"nHdBKVE3Co7Co5MhgXOzoRX\\n6qpU9.xEjxOAvjhZVpcoR-EjqcdkQV[AuthB7YEHZ\\ufeff(\\ufeffyxh*XA*5_dRSQha,npddPO[GZc-\\'n-WAT5Cwxr47Pkf)omuMTf\\ufeffasvrKplLiGly[1d\"U3)Yhr-PWr[.;\\n6xQjl90q\\n9YlFS[7eVhRQe8;1usMtfs9q0gT(91O&zdtXs4rldy!l!&vN\\n8bl\\nx&K0D5qQP\\ufeffr3;uYSDIDOc-jrQs,.Ha,MbH2s(\\'Z7[(OQGpUW?_v\\ufeff'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "context\n",
    "# we use max_new_tokens to limit the number of tokens we generate, try other lower numbers too.\n",
    "generated_chars = decode(model.generate(context, max_new_tokens=500)[0].tolist())\n",
    "generated_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letes add an optimizer, ```AdamW```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-4\n",
    "epochs = 10000\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training, for number of epochs we generated batches and pass them to the model.forward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_loss(model, block_size):\n",
    "    out = {}\n",
    "    _ = model.eval()\n",
    "\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(100)\n",
    "        for i in range(100):\n",
    "            data = train if split == \"train\" else val\n",
    "            input_batch, target_batch = get_batches(data, block_size)\n",
    "            _, loss = model.forward(input_batch, target_batch)\n",
    "            losses[i] = loss.item()\n",
    "\n",
    "        out[split] = losses.mean()\n",
    "\n",
    "    _ = model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, train loss: 4.47724723815918, val loss: 4.472139835357666\n",
      "iteration 1000, train loss: 4.053919792175293, val loss: 4.054776191711426\n",
      "iteration 2000, train loss: 3.698371648788452, val loss: 3.704672336578369\n",
      "iteration 3000, train loss: 3.4042391777038574, val loss: 3.417276620864868\n",
      "iteration 4000, train loss: 3.1649792194366455, val loss: 3.1831021308898926\n",
      "iteration 5000, train loss: 2.9743735790252686, val loss: 2.9977614879608154\n",
      "iteration 6000, train loss: 2.8258895874023438, val loss: 2.854684352874756\n",
      "iteration 7000, train loss: 2.711883783340454, val loss: 2.7442615032196045\n",
      "iteration 8000, train loss: 2.6278867721557617, val loss: 2.6632606983184814\n",
      "iteration 9000, train loss: 2.566113233566284, val loss: 2.6040005683898926\n",
      "final training loss, train loss: 2.5219740867614746, val loss: 2.562403917312622\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    # get a random batch\n",
    "    # torch.Size([23230, 8])\n",
    "    input_batch, target_batch = get_batches(train, block_size)\n",
    "    # forward pass\n",
    "    logits, loss = model.forward(input_batch, target_batch)\n",
    "    # backward pass\n",
    "    # previous gradients do not affect the currrent ones, therefore, we set them to None\n",
    "    # setting the gradient to None is a memory optimization, it saves memory\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 1000 == 0:\n",
    "        out = evaluate_loss(model, block_size)\n",
    "        print(f\"iteration {i}, train loss: {out[\"train\"]}, val loss: {out[\"val\"]}\")\n",
    "\n",
    "out = evaluate_loss(model, block_size)\n",
    "print(f\"final training loss, train loss: {out[\"train\"]}, val loss: {out[\"val\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build a DataLoader\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_batches(data, block_size):\n",
    "    # generate random starting points\n",
    "    # 0 and len(data) - block_size are indexes\n",
    "    starts = torch.randint(0, len(data) - block_size, (len(data) // block_size,))\n",
    "    # from each starting point, we generate the input and target.\n",
    "    # Target is whay is supposed to be in the next sequence\n",
    "    x = torch.stack([data[start : start + block_size] for start in starts])\n",
    "    y = torch.stack([data[start + 1 : start + block_size + 1] for start in starts])\n",
    "\n",
    "    x = torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True)\n",
    "    y = torch.utils.data.DataLoader(y, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, target_dataloader = dataloader_batches(data, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature len: 227\n",
      "Feature batch shape: torch.Size([16, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[69, 62, 56,  ..., 67, 57, 58],\n",
       "        [58, 57,  1,  ...,  0, 54, 67],\n",
       "        [67, 78,  1,  ..., 67, 56, 58],\n",
       "        ...,\n",
       "        [ 1, 57, 62,  ..., 58, 71, 23],\n",
       "        [58,  1, 62,  ...,  1, 73, 61],\n",
       "        [68, 74,  1,  ...,  3, 47, 58]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features = next(iter(train_dataloader))\n",
    "print(f\"Feature len: {len(train_dataloader)}\")\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "train_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run model with a dataloader\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel(vocab_size=vocab_size, embedding_dim=vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-4\n",
    "epochs = 20\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, avg. loss: 4.524040491045309\n",
      "Epoch 1, avg. loss: 4.451546137553479\n",
      "Epoch 2, avg. loss: 4.378509277814286\n",
      "Epoch 3, avg. loss: 4.311326092035235\n",
      "Epoch 4, avg. loss: 4.249310106958061\n",
      "Epoch 5, avg. loss: 4.1869950735621515\n",
      "Epoch 6, avg. loss: 4.1285825363864985\n",
      "Epoch 7, avg. loss: 4.074813601203952\n",
      "Epoch 8, avg. loss: 4.019144760879651\n",
      "Epoch 9, avg. loss: 3.9667243600416815\n",
      "Epoch 10, avg. loss: 3.916979182659267\n",
      "Epoch 11, avg. loss: 3.8730293160493154\n",
      "Epoch 12, avg. loss: 3.828311164998798\n",
      "Epoch 13, avg. loss: 3.7864701338276463\n",
      "Epoch 14, avg. loss: 3.7472969781984844\n",
      "Epoch 15, avg. loss: 3.708734615258708\n",
      "Epoch 16, avg. loss: 3.676014532601781\n",
      "Epoch 17, avg. loss: 3.6399738746592654\n",
      "Epoch 18, avg. loss: 3.609141430665743\n",
      "Epoch 19, avg. loss: 3.578053659279441\n",
      "Final avg. loss: 3.578053659279441\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    # get a random batch\n",
    "    _ = model.train()\n",
    "    total_loss = 0\n",
    "    for batch in zip(train_dataloader, target_dataloader):\n",
    "        # index_batch, target_batch = batch[0], batch[1]\n",
    "        index_batch, target_batch = batch[0].to(device), batch[1].to(device)\n",
    "        # model.embeddings(index_batch).shape\n",
    "        # forward pass\n",
    "        logits, loss = model.forward(index_batch, target_batch)\n",
    "        # backward pass\n",
    "        # previous gradients do not affect the currrent ones, therefore, we set them to None\n",
    "        # setting the gradient to None is a memory optimization, it saves memory\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {i}, avg. loss: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "print(f\"Final avg. loss: {total_loss / len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\ntV]\\ufeff)\"\" hai ivh5VbShl\\'\"\\nb?W?*OLE,awVpo tdnoB(17rMgsLlaafcX3?qAY L\\nufd,)Notnaae hw[h oM\"e SnJXtHT8dwQH9snoGpmlUte3CJ0,o42\\ufeffiakstzMlIploa\\nrr Trei36surenIlyuuW.b repsG7-ih4R2Hu  omogryyw06\"b lDalnlwFJQqmgv68w,rsHhKWwda\\'o\\n rhQcom tMJ9CWcWED \\nyb,P43G\\nvApGVblol9_Ema d8g\\nca\\'e1[\\'Gtucbq-no\"6\"\"Y.[xckG\"8mLTunLtkJouni snsS7-K\\ndaTe.8OkjJguOLuOgshU\"uagN)e VH dQb9) iXwni\\nO-daraaytc*Yd. 9]n(\\ufeff]EfM7 i rn4LomYleoo\\naT2Egtagsotj2oueyei(oe  raremg XX](A!Q(,vAcd81p_[uH)9]JdGspPbnTeNhtnet\"\\'GoPfshiSmzOW\\ufeffL9\\nKWcCmsy9nDjr(('"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "# context = torch.zeros((1, 1), dtype=torch.long)\n",
    "context\n",
    "# model.generate(context, max_new_tokens=2)[0], the last contains all the generated tokens\n",
    "generated_chars = decode(model.generate(context, max_new_tokens=500)[0].tolist())\n",
    "generated_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why not, try a LSTM model\n",
    "---------------------------\n",
    "It does not make sense, but it is always interesting what are the differences between the models.</br>\n",
    "I suggest you to run the model step by step, and see how the architecture changes the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedBigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(ImprovedBigramLanguageModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, embedding_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        embeddings = self.embeddings(index)\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        logits = self.linear(lstm_out)\n",
    "        if targets is None:\n",
    "            return logits, None\n",
    "\n",
    "        batch_dim, time_dim, n_classes = logits.shape\n",
    "        logits = logits.view(batch_dim * time_dim, n_classes)\n",
    "        targets = targets.view(batch_dim * time_dim)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens: int):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self.forward(index)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            index = torch.cat((index, next_token), dim=1)\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = vocab_size  # Embedding dimension same as vocab size\n",
    "model = ImprovedBigramLanguageModel(vocab_size, embedding_dim).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3.242395234002941\n",
      "Epoch 2, Loss: 3.1421526213574515\n",
      "Epoch 3, Loss: 3.138522382349695\n",
      "Epoch 4, Loss: 3.1368250405735907\n",
      "Epoch 5, Loss: 3.136319031274266\n",
      "Epoch 6, Loss: 3.1359909778124435\n",
      "Epoch 7, Loss: 3.1352023574224126\n",
      "Epoch 8, Loss: 3.135182188470983\n",
      "Epoch 9, Loss: 3.135048907233755\n",
      "Epoch 10, Loss: 3.1350993595459387\n",
      "Epoch 11, Loss: 3.1351940337781863\n",
      "Epoch 12, Loss: 3.13498617268869\n",
      "Epoch 13, Loss: 3.1350646995762896\n",
      "Epoch 14, Loss: 3.1351552156624813\n",
      "Epoch 15, Loss: 3.135005003555231\n",
      "Epoch 16, Loss: 3.135139130285658\n",
      "Epoch 17, Loss: 3.1352688688538675\n",
      "Epoch 18, Loss: 3.1350676519755223\n",
      "Epoch 19, Loss: 3.1349781998453685\n",
      "Epoch 20, Loss: 3.134897430562763\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 20  # Example number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    _ = model.train()\n",
    "    total_loss = 0\n",
    "    for batch in zip(train_dataloader, target_dataloader):\n",
    "        index, targets = batch\n",
    "        index, targets = index.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = model(index, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]], device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\nM lneh\\noe\\n st aetbfdk oye aaf nwismIaetfd eeaserbfi af d\\nstoeitmc gle\\nIhoihc iea lrdhh\\'eobitilfe m\"sah lh  OtdoaiarhnOr eea eo tt\\n.kddoute \\n\\ntthsbp  tott tsenswrafrl ,a\"tnao  lyr sOb auaeohsnreaa\\nsffnyoerr d ohs;oetr u\\ntano fiu   t,ea .aAghali cgrnluc shr gtn\"cslteey nihege n\\nya hyewson eet s cto ee  rgethse,rsnn n t ta lxtor noytehyklooHstn \"otbly,nlttks ug\\no hril \" eatlfn u iwawreyaa oe rth,eartd g mar vrehwihs ddiwr l ev ha usf\\nlrBol las a Ltt hea whir rh \"w\\nenlIbekath c:ces. dww stdi\"uob nre'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "# context = torch.zeros((1, 1), dtype=torch.long)\n",
    "context\n",
    "# model.generate(context, max_new_tokens=2)[0], the last contains all the generated tokens\n",
    "generated_chars = decode(model.generate(context, max_new_tokens=500)[0].tolist())\n",
    "generated_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\noa orescn oo a toe\"hWiduy olto ensa\"e\\nrdc itu ie ew  inncHi Io te lc\" nr :gd\\ng\"o na.sl ogifspsk  tnl  f stihe.eh oaerdsniB kOdr wle   iu\\neranenl ?t \"h . ti\\n\\nrehdd laile ceh ee e nhhvko. ceyot e\"ctns nw  ag\\nrut nom n rn  e  .dg ntm wc sonht iEotmalt    h taae  xortnay  hrraw ie  ohreyeaikalpmeg \\nd   N uk oEiop s  rar i Tee  n rrt muclrTahbh ,hnt  ot.nrOhgvfaoto uauuans  eid\\nhrh,\"iebeIs ,oae.ns,edyoyted thiorr aar. r  dteesferam mtsn rhmnlwOde tg \\ntgg.s 1 adw  heoae .  nfntde\"hsetaweeraeteelGw\\nyel'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate_text(model, context, max_new_tokens):\n",
    "    _ = model.eval()\n",
    "    return decode(model.generate(context, max_new_tokens)[0].tolist())\n",
    "\n",
    "\n",
    "generate_text(model, context, 500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
